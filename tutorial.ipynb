{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc8dd76",
   "metadata": {
    "id": "7dc8dd76"
   },
   "source": [
    "# AirPodsを活用した顎関節症判定チャレンジ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bbdc9",
   "metadata": {
    "id": "b47bbdc9"
   },
   "source": [
    "これは「AirPodsを活用した顎関節症判定チャレンジ」の分析・モデリングチュートリアルである. 分析・モデリングをする際の参考にされたい."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9199988",
   "metadata": {
    "id": "a9199988",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93728373",
   "metadata": {
    "id": "93728373",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555242e",
   "metadata": {
    "id": "b555242e"
   },
   "source": [
    "配布されている`train.zip`, `test.zip`をダウンロードし, このノートブックと同じディレクトリに配置して, zipファイルは解凍する. 解凍後以下のようなディレクトリができていることを確認.\n",
    "\n",
    "```\n",
    ".\n",
    "├── train              # 学習用データ\n",
    "│   ├── negative\n",
    "│   └── positive\n",
    "├── test               # 評価用データ\n",
    "│   ├── 000\n",
    "│   └── ...\n",
    "└── tutorial.ipynb     # このノートブックファイル\n",
    "```\n",
    "\n",
    "各データの定義については配布されている`README.pdf`を参照すること."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZzUwgkbArO8Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzUwgkbArO8Z",
    "outputId": "25189f20-7684-40e4-8c0e-ba97353d986d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9ff32",
   "metadata": {
    "id": "acd9ff32"
   },
   "source": [
    "#### Google Colaboratoryを使う場合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5df07b",
   "metadata": {
    "id": "0a5df07b"
   },
   "source": [
    "自身のドライブなどにこのjupyternotebookファイルをアップロードして立ち上げることでGoogle Colaboratoryを起動し, `/content`以下に`train.zip`, `test.zip`をアップロードする. その後下記コマンドを実行することで, zipファイルは解凍される."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905e190",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f905e190",
    "lines_to_next_cell": 2,
    "outputId": "2e63f124-7d99-4706-fe78-926332aa2b53"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/YIL-challenge-2/test.zip\" -d \"/content\"\n",
    "!unzip \"/content/drive/MyDrive/YIL-challenge-2/train.zip\"  -d \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50321c39",
   "metadata": {
    "id": "50321c39",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ライブラリのインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22da5a",
   "metadata": {
    "id": "cf22da5a"
   },
   "source": [
    "これから行う分析やモデリングに必要なライブラリをインストールする. 主に必要なライブラリは以下の通り.\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- torch\n",
    "- scipy\n",
    "- scikit-learn\n",
    "- matplotlib\n",
    "- skorch\n",
    "\n",
    "インストールされていないなら下記コマンドでインストールすること."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d6c02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c53d6c02",
    "outputId": "829cb2fa-8178-49c5-c008-dc534f04d031"
   },
   "outputs": [],
   "source": [
    "! pip install aeon scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dd194",
   "metadata": {
    "id": "ef6dd194",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ライブラリのインポートと設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c21ccd",
   "metadata": {
    "id": "46c21ccd"
   },
   "source": [
    "今後の作業で必要になる「道具」（＝Pythonのライブラリ）を用意する. また, データやファイルを保存しているフォルダの場所（ルートディレクトリ）を決めておく. これを最初にやっておくと, 今後データを読み込んだり保存したりするときに, 毎回場所を指定しなくてすむので便利である.\n",
    "\n",
    "下記セルのコメントアウトにてインポートしているライブラリの役割を簡単に確認されたい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6c47a8",
   "metadata": {
    "id": "df6c47a8"
   },
   "outputs": [],
   "source": [
    "# データ分析や機械学習に使う道具（ライブラリ）を準備します\n",
    "\n",
    "import os                          # ファイルやフォルダの操作を行う標準ライブラリ\n",
    "import pandas as pd                # 表形式データ（DataFrame）を扱う。データの読み書きや加工が得意\n",
    "import numpy as np                 # 高速な数値計算や行列・配列操作に使う\n",
    "import matplotlib.pyplot as plt    # グラフや可視化を行う\n",
    "\n",
    "from scipy.signal import resample  # 信号処理で使うリサンプリング（データ数変更）機能を利用可能\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # データを訓練用・テスト用に分割する関数\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score  # ROC曲線とAUCスコア（評価指標）計算に使う\n",
    "\n",
    "import torch                       # PyTorch本体\n",
    "import torch.nn as nn              # ニューラルネットワークの各種層\n",
    "import torch.nn.functional as F    # 活性化関数など\n",
    "from torch.utils.data import DataLoader, TensorDataset  # データローダー\n",
    "\n",
    "import pickle  # モデルの保存・読み込み用\n",
    "\n",
    "\n",
    "# データの読み込み場所, 結果の保存場所を決めておきます\n",
    "DATA_DIR = '.'     # データファイルがあるフォルダ（ここではカレントフォルダを指定. Google Colaboratoryを使っていてGoogle Driveをマウントしてデータを展開しているなら`/content/drive/MyDrive`などと設定）\n",
    "OUTPUT_DIR = '.'   # 結果などを保存するフォルダ（ここではカレントフォルダを指定. Google Colaboratoryを使っていてGoogle Driveをマウントしてデータを展開しているなら`/content/drive/MyDrive`などと設定）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93b155",
   "metadata": {
    "id": "fa93b155",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## データの整理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884b431",
   "metadata": {
    "id": "1884b431"
   },
   "source": [
    "これから分析に使うためのデータファイル（主にCSVファイル）の情報を整理して, いろいろな分析をできるようにする."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0899719",
   "metadata": {
    "id": "b0899719",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 学習用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523470f",
   "metadata": {
    "id": "4523470f"
   },
   "source": [
    "学習用として与えられているのは`./train`以下にある被験者ごとのairpodsより取得した系列データである. 以下の処理を行う.\n",
    "\n",
    "- 分類で使うラベル名(\"positive\")とファイルパス(\"fpath\"), 被験者ID(\"person\"), セット数(\"set_id\"), 動作の種類(\"move\"), 左右どちらか(\"leftorright\")を対応させて定義する.\n",
    "    - ラベル名は\"顎関節症の自覚症状がある\"=1, \"顎関節症の自覚症状がない\"=0\n",
    "    - 動作の種類は\"自力最大開口\"='01', \"側転運動\"='02'\n",
    "    - 左右どちらかは'left'=左耳から取得, 'right'=右耳から取得\n",
    "- 被験者の数やセット回数などを確認する.\n",
    "\n",
    "これらは分析や可視化を行うための準備である."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923755bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "923755bb",
    "outputId": "2f6e4411-cc55-4861-a5aa-39ccb4e2ecfa"
   },
   "outputs": [],
   "source": [
    "# データファイルの場所（ディレクトリ名：train）を指定します\n",
    "data_path = os.path.join(DATA_DIR, 'train')   # DATA_DIRは事前にどこかで定義されている前提\n",
    "\n",
    "train_master = []    # メタデータ（ファイルパスや属性情報）をリストで保持\n",
    "\n",
    "# trainフォルダ以下に保存された全データ（階層構造）を探索\n",
    "for label_name in os.listdir(data_path):  # label_name: ラベル（'positive'または'negative'のフォルダ）\n",
    "    for person_id in os.listdir(os.path.join(data_path, label_name)):  # person_id: 被験者ID（サブディレクトリ）\n",
    "        for file_name in os.listdir(os.path.join(data_path, label_name, person_id)):  # file_name: 実際のデータファイル名\n",
    "            # ファイル名を_で区切って各属性を抽出（例: \"set01_01_left.csv\" なら set_id=\"01\", move=\"01\", leftorright=\"left\"）\n",
    "            set_id, move, leftorright = file_name.split('.')[0].split('_')\n",
    "\n",
    "            # 各データについて、ファイルパスや被験者IDなどの情報をまとめて辞書にする\n",
    "            train_master.append({\n",
    "                'fpath': os.path.join(data_path, label_name, person_id, file_name),   # ファイルのフルパス\n",
    "                'person': person_id,                                                  # 被験者ID\n",
    "                'set_id': set_id,                                                     # セットID（同じ動作/回の区別）\n",
    "                'move': move,                                                         # 動作名\n",
    "                'leftorright': leftorright,                                           # 左右情報\n",
    "                'positive': 1 if label_name == 'positive' else 0                      # ラベル（positive=1, negative=0）\n",
    "            })\n",
    "\n",
    "# リスト（train_master）をpandasのDataFrameに変換\n",
    "train_master = pd.DataFrame(train_master)\n",
    "\n",
    "# DataFrameの最初の数行を表示して、正しくデータを作れているか確認\n",
    "print(train_master.head())\n",
    "\n",
    "# 重複を除いた被験者（person）数を表示\n",
    "print('\\n被験者の数:', len(train_master['person'].unique()))\n",
    "\n",
    "# 各被験者ごとに持っているセットIDの数を集計し、平均値を表示\n",
    "print(\n",
    "    '被験者ごとのセット回数:',\n",
    "    train_master.groupby('person').apply(\n",
    "        lambda x: len(x['set_id'].unique()), include_groups=False # type: ignore\n",
    "    ).describe()['mean']   # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f2c7f",
   "metadata": {
    "id": "ee4f2c7f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 評価用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3507879",
   "metadata": {
    "id": "c3507879"
   },
   "source": [
    "評価用として与えられているのは`./test`以下にあるairpodsの系列データのみである. 各被験者, セット回数(具体的に与えられるわけではない)ごとに2種類の動作(自力最大開口, 側方運動)と左右の組み合わせで計4パターンの測定結果がcsvファイルとして格納されている. なお, 学習用とは別の被験者のデータであることに注意. ラベル情報は与えられず, このラベルを当てることが今回の課題の主目的である. ここでは学習用データと同様に以下の処理を行う.\n",
    "\n",
    "- 評価用データは被験者, セット回数(ディレクトリ名としてIDが振られている)ごとにまとめられているので, それぞれの対応するデータのファイルパス(\"fpath\")を紐づける.\n",
    "    - 各IDごとに4つの系列データファイルが紐づくことになる.\n",
    "- 推論の対象となるサンプル数を確認する. 各IDごとに推論を行うことになる.\n",
    "\n",
    "これらは学習した機械学習モデルによって評価用データに対して推論を行って, 結果を作成するための準備である."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3928e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3928e9",
    "outputId": "dc5816bb-8061-4f33-d43d-53a52fcb879c"
   },
   "outputs": [],
   "source": [
    "# テストデータのファイル情報を記録するリスト\n",
    "test_master = []\n",
    "\n",
    "# testディレクトリ配下の各サブディレクトリ（ID単位）を探索\n",
    "for test_id in os.listdir(os.path.join(DATA_DIR, 'test')):  # test_id: テストID\n",
    "    for file_name in os.listdir(os.path.join(DATA_DIR, 'test', test_id)):  # file_name: データファイル名\n",
    "        # 各ファイルごとに「id」とファイルのフルパスを記録\n",
    "        test_master.append({\n",
    "            'id': test_id,  # サンプルID（被験者IDなど）\n",
    "            'fpath': os.path.join(DATA_DIR, 'test', test_id, file_name)  # ファイルのフルパス\n",
    "        })\n",
    "\n",
    "# リストをDataFrameに変換して管理しやすくする\n",
    "test_master = pd.DataFrame(test_master)\n",
    "\n",
    "# データフレームの先頭5行を表示（正しく格納されているか確認）\n",
    "print(test_master.head())\n",
    "\n",
    "# テストデータのID（重複なし）の数を表示\n",
    "print('\\nサンプル数:', len(test_master['id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxj74RBL1Wb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxj74RBL1Wb3",
    "lines_to_next_cell": 2,
    "outputId": "50a8f728-b2d2-4fcc-b98c-5fe38aca4271"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, ttest_ind\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import rfft\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ======================\n",
    "# 対象チャネル: 全チャネルを使用\n",
    "# ======================\n",
    "\n",
    "# (CSVの全カラムを自動的に使用)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 特徴抽出関数\n",
    "# ======================\n",
    "\n",
    "def compute_features(signal):\n",
    "\n",
    "    feats = {}\n",
    "    x = signal\n",
    "    n = len(x)\n",
    "    dx = np.diff(x)\n",
    "    ddx = np.diff(x, n=2)\n",
    "\n",
    "    # 長さ非依存の統計量\n",
    "    feats['mean'] = np.mean(x)\n",
    "    feats['std'] = np.std(x)\n",
    "    feats['median'] = np.median(x)\n",
    "    feats['p5'] = np.percentile(x, 5)\n",
    "    feats['p10'] = np.percentile(x, 10)\n",
    "    feats['p25'] = np.percentile(x, 25)\n",
    "    feats['p75'] = np.percentile(x, 75)\n",
    "    feats['p90'] = np.percentile(x, 90)\n",
    "    feats['p95'] = np.percentile(x, 95)\n",
    "    feats['iqr'] = feats['p75'] - feats['p25']\n",
    "    feats['idr'] = feats['p90'] - feats['p10']  # interdecile range\n",
    "    feats['ptp'] = np.ptp(x)\n",
    "    feats['skew'] = skew(x)\n",
    "    feats['kurt'] = kurtosis(x)\n",
    "    feats['rms'] = np.sqrt(np.mean(x**2))\n",
    "    feats['tv'] = np.mean(np.abs(dx))       # sum → mean に変更\n",
    "    feats['jerk_std'] = np.std(ddx) if len(ddx)>0 else 0\n",
    "\n",
    "    peaks,_ = find_peaks(np.abs(x))\n",
    "    feats['n_peaks'] = len(peaks) / n       # 長さで正規化\n",
    "\n",
    "    feats['energy'] = np.mean(x**2)         # sum → mean に変更\n",
    "\n",
    "    fft_vals = np.abs(rfft(x))\n",
    "    n_fft = len(fft_vals)\n",
    "    feats['fft_low'] = np.mean(fft_vals[:5]) if n_fft >= 5 else 0     # sum → mean\n",
    "    feats['fft_mid'] = np.mean(fft_vals[5:15]) if n_fft >= 15 else 0  # sum → mean\n",
    "    feats['fft_high'] = np.mean(fft_vals[15:]) if n_fft > 15 else 0   # sum → mean\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 全特徴生成 (左右差 + 相関 + 比率 + 個別チャネル)\n",
    "# ======================\n",
    "\n",
    "all_features = []\n",
    "labels = []\n",
    "persons = []\n",
    "\n",
    "for (person, set_id, positive), group in train_master.groupby(['person','set_id','positive']):\n",
    "\n",
    "    row_feats = {}\n",
    "\n",
    "    for move in ['01','02']:\n",
    "\n",
    "        rows = group[group['move']==move]\n",
    "\n",
    "        left = None\n",
    "        right = None\n",
    "\n",
    "        for _, r in rows.iterrows():\n",
    "            df = pd.read_csv(r['fpath']).set_index('Timestamp')\n",
    "            df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "            if r['leftorright']=='left':\n",
    "                left = df\n",
    "            else:\n",
    "                right = df\n",
    "\n",
    "        if left is None or right is None:\n",
    "            continue\n",
    "\n",
    "        # 共通カラムのみ使用\n",
    "        common_cols = [c for c in left.columns if c in right.columns]\n",
    "        left = left[common_cols]\n",
    "        right = right[common_cols]\n",
    "\n",
    "        n = min(len(left), len(right))\n",
    "        left = left.iloc[:n]\n",
    "        right = right.iloc[:n]\n",
    "\n",
    "        for i, col in enumerate(common_cols):\n",
    "            l_vals = left.iloc[:, i].values\n",
    "            r_vals = right.iloc[:, i].values\n",
    "\n",
    "            # ① 左右差の統計量\n",
    "            diff = l_vals - r_vals\n",
    "            feats = compute_features(diff)\n",
    "            for k, v in feats.items():\n",
    "                row_feats[f'{move}_{col}_diff_{k}'] = v\n",
    "\n",
    "            # ② 左右の相関係数\n",
    "            if len(l_vals) > 2 and np.std(l_vals) > 0 and np.std(r_vals) > 0:\n",
    "                corr = np.corrcoef(l_vals, r_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 0.0\n",
    "            row_feats[f'{move}_{col}_lr_corr'] = corr\n",
    "\n",
    "            # ③ 左右の比率 (比率の統計量)\n",
    "            ratio = l_vals / (np.abs(r_vals) + 1e-8)\n",
    "            row_feats[f'{move}_{col}_ratio_mean'] = np.mean(ratio)\n",
    "            row_feats[f'{move}_{col}_ratio_std'] = np.std(ratio)\n",
    "\n",
    "            # ④ 左チャネル単体の統計量\n",
    "            l_feats = compute_features(l_vals)\n",
    "            for k, v in l_feats.items():\n",
    "                row_feats[f'{move}_{col}_left_{k}'] = v\n",
    "\n",
    "            # ⑤ 右チャネル単体の統計量\n",
    "            r_feats = compute_features(r_vals)\n",
    "            for k, v in r_feats.items():\n",
    "                row_feats[f'{move}_{col}_right_{k}'] = v\n",
    "\n",
    "    # ⑦ 動作間の差分 (01 vs 02)\n",
    "    move01_keys = [k for k in row_feats if k.startswith('01_')]\n",
    "    for k01 in move01_keys:\n",
    "        k02 = '02_' + k01[3:]  # 01_ → 02_\n",
    "        if k02 in row_feats:\n",
    "            suffix = k01[3:]   # 例: AttitudeRoll_diff_mean\n",
    "            row_feats[f'move_diff_{suffix}'] = row_feats[k01] - row_feats[k02]\n",
    "\n",
    "    if len(row_feats)>0:\n",
    "        all_features.append(row_feats)\n",
    "        labels.append(positive)\n",
    "        persons.append(person)\n",
    "\n",
    "\n",
    "X = pd.DataFrame(all_features)\n",
    "y = np.array(labels)\n",
    "groups = np.array(persons)\n",
    "\n",
    "print(f\"特徴量数: {X.shape[1]}, サンプル数: {X.shape[0]}, 被験者数: {len(np.unique(groups))}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 統計検定 (被験者単位 + FDR補正)\n",
    "# ======================\n",
    "\n",
    "# 被験者単位で集約 (独立性を確保)\n",
    "X_person = X.copy()\n",
    "X_person['person'] = groups\n",
    "X_person['label'] = y\n",
    "X_person = X_person.groupby('person').agg(\n",
    "    {col: 'mean' for col in X.columns} | {'label': 'first'}\n",
    ")\n",
    "\n",
    "y_person = X_person['label'].values\n",
    "feature_columns = [c for c in X_person.columns if c != 'label']\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in feature_columns:\n",
    "\n",
    "    pos = X_person[y_person==1][col].dropna()\n",
    "    neg = X_person[y_person==0][col].dropna()\n",
    "\n",
    "    if len(pos)<3 or len(neg)<3:\n",
    "        continue\n",
    "\n",
    "    t, p = ttest_ind(pos, neg, equal_var=False)\n",
    "    d = (pos.mean()-neg.mean()) / np.sqrt((pos.var()+neg.var())/2)\n",
    "\n",
    "    results.append([col, p, d])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['feature','p_raw','cohen_d'])\n",
    "\n",
    "# FDR補正 (Benjamini-Hochberg法)\n",
    "rejected, p_corrected, _, _ = multipletests(results_df['p_raw'], method='fdr_bh')\n",
    "results_df['p_fdr'] = p_corrected\n",
    "results_df['significant'] = rejected\n",
    "\n",
    "# StratifiedGroupKFold AUC (リークなし)\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "auc_results = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    fold_aucs = []\n",
    "    for train_idx, test_idx in cv.split(X, y, groups=groups):\n",
    "        test_labels = y[test_idx]\n",
    "        if len(np.unique(test_labels)) < 2:\n",
    "            continue\n",
    "        auc = roc_auc_score(test_labels, X.iloc[test_idx][col])\n",
    "        fold_aucs.append(auc)\n",
    "    if len(fold_aucs)>0:\n",
    "        auc_results.append([col, np.mean(fold_aucs)])\n",
    "\n",
    "auc_df = pd.DataFrame(auc_results, columns=['feature','cv_auc'])\n",
    "\n",
    "# 結合してランキング\n",
    "results_df = results_df.merge(auc_df, on='feature', how='left')\n",
    "results_df = results_df.sort_values('cv_auc', ascending=False)\n",
    "\n",
    "print(\"\\n=== 特徴量ランキング (被験者単位t検定 + FDR補正 + CV-AUC) ===\")\n",
    "print(results_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tjvmNn4J2iUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwd_selection",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Forward Feature Selection (貪欲探索)\n",
    "# ======================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# === CV方式の選択 ===\n",
    "# 'LOPO' : Leave-One-Person-Out (21 folds, 再現性あり, 保守的)\n",
    "# 'RSKF' : Repeated StratifiedGroupKFold (10 seeds × 5 folds)\n",
    "CV_MODE = 'LOPO'\n",
    "\n",
    "# === 正則化パラメータ ===\n",
    "# C が小さいほど正則化が強い (デフォルト: 1.0)\n",
    "REG_C = 0.05\n",
    "\n",
    "def cv_auc(X_mat, y_vec, groups_vec):\n",
    "    \"\"\"CV_MODEに応じたAUCを計算\"\"\"\n",
    "    if CV_MODE == 'LOPO':\n",
    "        logo = LeaveOneGroupOut()\n",
    "        all_probs = np.zeros(len(y_vec))\n",
    "        for train_idx, test_idx in logo.split(X_mat, y_vec, groups=groups_vec):\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X_mat[train_idx])\n",
    "            X_te = scaler.transform(X_mat[test_idx])\n",
    "            model = LogisticRegression(C=REG_C)\n",
    "            model.fit(X_tr, y_vec[train_idx])\n",
    "            all_probs[test_idx] = model.predict_proba(X_te)[:, 1]\n",
    "        return roc_auc_score(y_vec, all_probs)\n",
    "    else:  # RSKF\n",
    "        seed_means = []\n",
    "        for seed in range(42, 52):\n",
    "            cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "            fold_aucs = []\n",
    "            for train_idx, test_idx in cv.split(X_mat, y_vec, groups=groups_vec):\n",
    "                y_test = y_vec[test_idx]\n",
    "                if len(np.unique(y_test)) < 2:\n",
    "                    continue\n",
    "                scaler = StandardScaler()\n",
    "                X_tr = scaler.fit_transform(X_mat[train_idx])\n",
    "                X_te = scaler.transform(X_mat[test_idx])\n",
    "                model = LogisticRegression(C=REG_C)\n",
    "                model.fit(X_tr, y_vec[train_idx])\n",
    "                prob = model.predict_proba(X_te)[:, 1]\n",
    "                fold_aucs.append(roc_auc_score(y_test, prob))\n",
    "            if fold_aucs:\n",
    "                seed_means.append(np.mean(fold_aucs))\n",
    "        return np.mean(seed_means) if seed_means else 0.0\n",
    "\n",
    "print(f\"CV方式: {CV_MODE}\")\n",
    "\n",
    "# 候補: ランキング上位50個に絞る (計算時間削減)\n",
    "TOP_K = 300\n",
    "candidates = results_df.head(TOP_K)['feature'].tolist()\n",
    "\n",
    "y_arr = np.asarray(y)\n",
    "g_arr = np.asarray(groups)\n",
    "\n",
    "selected = []\n",
    "best_score = 0.0\n",
    "MAX_FEATURES = 3  # 最大特徴数\n",
    "\n",
    "print(f\"候補特徴量: {len(candidates)}個 → 最大{MAX_FEATURES}個を選択\\n\")\n",
    "\n",
    "for step in range(MAX_FEATURES):\n",
    "    best_feat = None\n",
    "    best_new_score = best_score\n",
    "\n",
    "    for feat in candidates:\n",
    "        if feat in selected:\n",
    "            continue\n",
    "\n",
    "        trial = selected + [feat]\n",
    "        X_trial = X[trial].values\n",
    "        score = cv_auc(X_trial, y_arr, g_arr)\n",
    "\n",
    "        if score > best_new_score:\n",
    "            best_new_score = score\n",
    "            best_feat = feat\n",
    "\n",
    "    if best_feat is None:\n",
    "        print(f\"\\nStep {step+1}: 改善する特徴なし → 終了\")\n",
    "        break\n",
    "\n",
    "    selected.append(best_feat)\n",
    "    best_score = best_new_score\n",
    "    print(f\"Step {step+1}: + {best_feat}  → AUC = {best_score:.4f}\")\n",
    "\n",
    "print(f\"\\n=== 選択された特徴量 ({len(selected)}個) ===\")\n",
    "for f in selected:\n",
    "    print(f\"  - {f}\")\n",
    "print(f\"最終 AUC: {best_score:.4f}\")\n",
    "\n",
    "FEATURE_NAMES = selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedab60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGvOzIP64Jn8",
    "lines_to_next_cell": 2,
    "outputId": "b8929fad-d7e4-45e3-fe26-7f0d48703ecb"
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# CV 評価 (選択された特徴量)\n",
    "# ======================\n",
    "\n",
    "print(f\"使用特徴量: {len(FEATURE_NAMES)}個\")\n",
    "for f in FEATURE_NAMES:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "X_cv = X[FEATURE_NAMES].values\n",
    "y_cv = np.asarray(y)\n",
    "groups_cv = np.asarray(groups)\n",
    "\n",
    "if CV_MODE == 'LOPO':\n",
    "    print(f\"\\n--- LOPO CV (21 folds) ---\")\n",
    "    logo = LeaveOneGroupOut()\n",
    "    lopo_probs = np.zeros(len(y_cv))\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X_cv, y_cv, groups=groups_cv):\n",
    "        X_train, X_test = X_cv[train_idx], X_cv[test_idx]\n",
    "        y_train, y_test = y_cv[train_idx], y_cv[test_idx]\n",
    "        person = groups_cv[test_idx][0]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegression(C=REG_C)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        prob = model.predict_proba(X_test)[:, 1]\n",
    "        lopo_probs[test_idx] = prob\n",
    "\n",
    "        label_str = \"pos\" if y_test[0] == 1 else \"neg\"\n",
    "        print(f\"  Person {person} ({label_str}): mean prob = {np.mean(prob):.4f}\")\n",
    "\n",
    "    lopo_auc = roc_auc_score(y_cv, lopo_probs)\n",
    "    print(f\"\\n=== LOPO AUC: {lopo_auc:.4f} ===\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n--- Repeated StratifiedGroupKFold ---\")\n",
    "    SEEDS = list(range(42, 52))\n",
    "    all_aucs = []\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        seed_aucs = []\n",
    "\n",
    "        for train_idx, test_idx in cv.split(X_cv, y_cv, groups=groups_cv):\n",
    "            X_train, X_test = X_cv[train_idx], X_cv[test_idx]\n",
    "            y_train, y_test = y_cv[train_idx], y_cv[test_idx]\n",
    "\n",
    "            if len(np.unique(y_test)) < 2:\n",
    "                continue\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            model = LogisticRegression(C=REG_C)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            seed_aucs.append(auc)\n",
    "\n",
    "        if len(seed_aucs) > 0:\n",
    "            seed_mean = np.mean(seed_aucs)\n",
    "            all_aucs.append(seed_mean)\n",
    "            fold_str = \", \".join([f\"{a:.3f}\" for a in seed_aucs])\n",
    "            print(f\"  Seed {seed}: Mean AUC = {seed_mean:.4f}  [{fold_str}]  ({len(seed_aucs)} folds)\")\n",
    "\n",
    "    print(f\"\\n=== Repeated CV ({len(all_aucs)} valid seeds) ===\")\n",
    "    print(f\"Mean AUC: {np.mean(all_aucs):.4f} (±{np.std(all_aucs):.4f})\")\n",
    "    print(f\"Min: {np.min(all_aucs):.4f}, Max: {np.max(all_aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TZJzoX9U3kK4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCpvtWrN5OHe",
    "lines_to_next_cell": 2,
    "outputId": "4ac6a8bb-f813-4d32-d612-42cba80cc87a"
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 提出用: 全データで学習 → テスト予測\n",
    "# ======================\n",
    "\n",
    "TEST_ROOT = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# --- 学習 (set単位で全データ使用) ---\n",
    "X_train_final = X[FEATURE_NAMES].values\n",
    "y_train_final = np.asarray(y)\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train_final)\n",
    "\n",
    "model_final = LogisticRegression(C=REG_C)\n",
    "model_final.fit(X_train_scaled, y_train_final)\n",
    "\n",
    "print(f\"学習完了: {len(X_train_final)}サンプル (set単位), 特徴量: {len(FEATURE_NAMES)}個\")\n",
    "\n",
    "\n",
    "# --- テスト特徴抽出 ---\n",
    "\n",
    "def extract_test_features(folder_path, feature_names):\n",
    "    \"\"\"テストデータから特徴を抽出 (学習と同じ5種類)\"\"\"\n",
    "    row_feats = {}\n",
    "\n",
    "    for move in ['01', '02']:\n",
    "        left_path = os.path.join(folder_path, f\"{move}_left.csv\")\n",
    "        right_path = os.path.join(folder_path, f\"{move}_right.csv\")\n",
    "\n",
    "        left = pd.read_csv(left_path).set_index(\"Timestamp\")\n",
    "        right = pd.read_csv(right_path).set_index(\"Timestamp\")\n",
    "\n",
    "        left = left[~left.index.duplicated()].sort_index()\n",
    "        right = right[~right.index.duplicated()].sort_index()\n",
    "\n",
    "        common_cols = [c for c in left.columns if c in right.columns]\n",
    "        left = left[common_cols]\n",
    "        right = right[common_cols]\n",
    "\n",
    "        n = min(len(left), len(right))\n",
    "        left = left.iloc[:n]\n",
    "        right = right.iloc[:n]\n",
    "\n",
    "        for i, col in enumerate(common_cols):\n",
    "            l_vals = left.iloc[:, i].values\n",
    "            r_vals = right.iloc[:, i].values\n",
    "\n",
    "            # ① 左右差\n",
    "            diff = l_vals - r_vals\n",
    "            feats = compute_features(diff)\n",
    "            for k, v in feats.items():\n",
    "                row_feats[f'{move}_{col}_diff_{k}'] = v\n",
    "\n",
    "            # ② 相関\n",
    "            if len(l_vals) > 2 and np.std(l_vals) > 0 and np.std(r_vals) > 0:\n",
    "                corr = np.corrcoef(l_vals, r_vals)[0, 1]\n",
    "            else:\n",
    "                corr = 0.0\n",
    "            row_feats[f'{move}_{col}_lr_corr'] = corr\n",
    "\n",
    "            # ③ 比率\n",
    "            ratio = l_vals / (np.abs(r_vals) + 1e-8)\n",
    "            row_feats[f'{move}_{col}_ratio_mean'] = np.mean(ratio)\n",
    "            row_feats[f'{move}_{col}_ratio_std'] = np.std(ratio)\n",
    "\n",
    "            # ④ 左単体\n",
    "            l_feats = compute_features(l_vals)\n",
    "            for k, v in l_feats.items():\n",
    "                row_feats[f'{move}_{col}_left_{k}'] = v\n",
    "\n",
    "            # ⑤ 右単体\n",
    "            r_feats = compute_features(r_vals)\n",
    "            for k, v in r_feats.items():\n",
    "                row_feats[f'{move}_{col}_right_{k}'] = v\n",
    "\n",
    "    # ⑦ 動作間の差分\n",
    "    move01_keys = [k for k in row_feats if k.startswith('01_')]\n",
    "    for k01 in move01_keys:\n",
    "        k02 = '02_' + k01[3:]\n",
    "        if k02 in row_feats:\n",
    "            suffix = k01[3:]\n",
    "            row_feats[f'move_diff_{suffix}'] = row_feats[k01] - row_feats[k02]\n",
    "\n",
    "    return [row_feats[f] for f in feature_names]\n",
    "\n",
    "\n",
    "# --- テスト予測 ---\n",
    "test_ids = sorted(os.listdir(TEST_ROOT))\n",
    "results = []\n",
    "\n",
    "for tid in test_ids:\n",
    "    folder = os.path.join(TEST_ROOT, tid)\n",
    "    feat_values = extract_test_features(folder, FEATURE_NAMES)\n",
    "    feat_scaled = scaler_final.transform([feat_values])\n",
    "    prob = model_final.predict_proba(feat_scaled)[0, 1]\n",
    "    results.append({\"id\": tid, \"positive\": prob})\n",
    "\n",
    "submission = pd.DataFrame(results)\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "submission.to_csv(\"submission.csv\", index=False, header=False)\n",
    "\n",
    "print(f\"\\n✓ submission.csv created ({len(submission)} samples)\")\n",
    "print(f\"  ID range: {submission['id'].iloc[0]} → {submission['id'].iloc[-1]}\")\n",
    "print(f\"  Prediction range: [{submission['positive'].min():.4f}, {submission['positive'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb50005",
   "metadata": {
    "id": "dbb50005",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 深層学習の実装時によくある注意点・Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2b7df",
   "metadata": {
    "id": "85f2b7df"
   },
   "source": [
    "主な工夫点は下記の通り. 実装にチャレンジして精度改善できるか試されたい.\n",
    "\n",
    "1. 特徴量拡張・データ拡張\n",
    "    - **追加特徴量の導入**  \n",
    "        - 加速度・角速度・クォータニオンなどを「差分」や「移動平均」「標準偏差」「エネルギー」などの統計量でも特徴量化\n",
    "        - 複数ウィンドウ幅での時系列要約・小さな区間の最大・最小・ピーク回数などを追加\n",
    "    - **ノイズ付与やシャッフルによるデータ拡張**  \n",
    "        - トレーニング時にランダムな小ノイズ/ランダム時刻シフト/部分欠損を加えて学習データを水増し（データ拡張）\n",
    "        - データミラー/左右反転が妥当なら適用する\n",
    "1. モデルアーキテクチャの改良\n",
    "    - **Conv1D層の深層化やカーネル幅・チャンネル数の調整**\n",
    "        - チャンネル数やフィルター幅を雑に数パターン試し, 最も安定するものを採用\n",
    "    - **畳み込み＋GRUやLSTMなどのRNN併用ハイブリッド**\n",
    "        - Conv後にGRU/LSTMを付加し「時系列での長い依存関係」も捉える\n",
    "    - **Attention層やSelf-Attention (Transformer的小規模モジュール) の組み込み**\n",
    "        - センサーデータの重要な時刻点を自動で強調\n",
    "    - **DropoutやLayerNorm, GELU等の正則化/活性化工夫**\n",
    "1. 前処理・入力の工夫\n",
    "    - **グローバルな標準化・正規化**\n",
    "        - サンプルごとの標準化から「学習用全体の平均・分散で標準化」への切り替え検討（一般化性能向上）.\n",
    "    - **入力ウィンドウ長や次元数の最適化**\n",
    "        - `seq_len`を増減して最適値探索.\n",
    "    - **欠損補完や異常値の明示的処理**\n",
    "1. 損失関数・評価・学習プロセス工夫\n",
    "    - **ロス関数を工夫する**（クラス不均衡ならFocal Loss, Weighted Lossの活用）\n",
    "    - **アーリーストッピング/EarlyStopping導入**（バリデーションロスやAUC見て自動で学習終了）\n",
    "    - **k-foldクロスバリデーションによる精度安定化評価**\n",
    "    - **LearningRateSchedulerの導入で動的に学習率調整**\n",
    "1. アンサンブル・他手法併用\n",
    "    - **同じ形式でLightGBMやCatBoost等の古典機械学習で特徴量を混ぜる**\n",
    "    - **複数モデルのアンサンブル（多数決/平均化）**\n",
    "1. 可視化とエラー解析\n",
    "    - **混同行列や確率分布図・ヒートマップでどの被験者/セットで間違えやすいか探索**\n",
    "    - **特徴量の重要度解析（Permutation Importanceなど）**\n",
    "1. ハイパーパラメータ探索\n",
    "    - **OptunaやGridSearchCV, RandomizedSearchCVによるモデルパラメータ自動最適化**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
